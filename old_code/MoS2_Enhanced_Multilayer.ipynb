{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Enhanced MoS2 Multilayer Detection Pipeline\n",
        "\n",
        "## Optimizations based on your feedback:\n",
        "- **Stage 1**: Working well (26 flakes detected) ‚úÖ\n",
        "- **Stage 2**: **ENHANCED** - Much more sensitive multilayer detection\n",
        "- **Stage 3**: Twist angle calculation for all detected multilayers\n",
        "\n",
        "## Stage 2 Improvements:\n",
        "- üîç **Lower area thresholds** (2% instead of 5%)\n",
        "- üîç **Multiple edge detection methods** with different sensitivities\n",
        "- üîç **Enhanced intensity analysis** (multiple thresholds)\n",
        "- üîç **Texture-based detection** for subtle internal features\n",
        "- üîç **Relaxed shape constraints** for irregular internal structures\n",
        "\n",
        "This should detect flakes #11, #14, and other multilayer candidates you identified!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install opencv-python-headless matplotlib numpy scipy scikit-image\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "from scipy import ndimage\n",
        "from skimage import measure, morphology, filters, feature\n",
        "from skimage.filters import gaussian, sobel, laplace\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('/content/images', exist_ok=True)\n",
        "os.makedirs('/content/results', exist_ok=True)\n",
        "os.makedirs('/content/debug', exist_ok=True)\n",
        "\n",
        "print(\"‚úì Environment setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced_analyzer"
      },
      "outputs": [],
      "source": [
        "class EnhancedMoS2Pipeline:\n",
        "    def __init__(self):\n",
        "        self.intensity_threshold = 140  # Stage 1 works well\n",
        "        self.min_flake_area = 200      \n",
        "        self.max_flake_area = 15000    \n",
        "        \n",
        "        # Enhanced Stage 2 parameters\n",
        "        self.min_internal_area = 80        # Reduced from 100\n",
        "        self.min_area_ratio = 0.02         # Reduced from 0.05 (2% instead of 5%)\n",
        "        self.intensity_drop_levels = [5, 10, 15, 20]  # Multiple sensitivity levels\n",
        "        \n",
        "    def stage1_detect_flakes(self, image_path):\n",
        "        \"\"\"Stage 1: Detect MoS2 flakes - SAME AS BEFORE (working well)\"\"\"\n",
        "        print(f\"\\n=== STAGE 1: FLAKE DETECTION ===\")\n",
        "        print(f\"Processing: {Path(image_path).name}\")\n",
        "        \n",
        "        # Load image\n",
        "        img = cv2.imread(image_path)\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "        \n",
        "        print(f\"Image intensity range: {gray.min()} - {gray.max()}\")\n",
        "        \n",
        "        # Apply optimized threshold (flakes are darker than background)\n",
        "        binary = (gray < self.intensity_threshold).astype(np.uint8) * 255\n",
        "        \n",
        "        # Clean up binary mask\n",
        "        kernel_open = np.ones((2,2), np.uint8)\n",
        "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_open)\n",
        "        \n",
        "        kernel_close = np.ones((4,4), np.uint8)\n",
        "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_close)\n",
        "        \n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        # Filter and analyze flakes\n",
        "        flakes = []\n",
        "        for contour in contours:\n",
        "            area = cv2.contourArea(contour)\n",
        "            \n",
        "            # Size filtering\n",
        "            if area < self.min_flake_area or area > self.max_flake_area:\n",
        "                continue\n",
        "            \n",
        "            # Shape analysis\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            if perimeter == 0:\n",
        "                continue\n",
        "                \n",
        "            # Approximate contour\n",
        "            epsilon = 0.02 * perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            \n",
        "            # Calculate shape metrics\n",
        "            hull = cv2.convexHull(contour)\n",
        "            hull_area = cv2.contourArea(hull)\n",
        "            solidity = area / hull_area if hull_area > 0 else 0\n",
        "            \n",
        "            # Aspect ratio\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            aspect_ratio = max(w, h) / min(w, h) if min(w, h) > 0 else 1\n",
        "            \n",
        "            # Circularity\n",
        "            circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
        "            \n",
        "            # Filter by shape (more permissive for MoS2 flakes)\n",
        "            is_valid_flake = (\n",
        "                0.3 < solidity < 1.0 and      # Allow some irregularity\n",
        "                aspect_ratio < 5.0 and        # Not too elongated\n",
        "                circularity > 0.15 and        # Not too thin/linear\n",
        "                3 <= len(approx) <= 10        # Polygonal shape\n",
        "            )\n",
        "            \n",
        "            if is_valid_flake:\n",
        "                # Calculate centroid\n",
        "                M = cv2.moments(contour)\n",
        "                cx = int(M['m10']/M['m00']) if M['m00'] != 0 else 0\n",
        "                cy = int(M['m01']/M['m00']) if M['m00'] != 0 else 0\n",
        "                \n",
        "                flakes.append({\n",
        "                    'id': len(flakes) + 1,\n",
        "                    'contour': contour,\n",
        "                    'approx': approx,\n",
        "                    'area': area,\n",
        "                    'perimeter': perimeter,\n",
        "                    'solidity': solidity,\n",
        "                    'aspect_ratio': aspect_ratio,\n",
        "                    'circularity': circularity,\n",
        "                    'vertices': len(approx),\n",
        "                    'centroid': (cx, cy),\n",
        "                    'bbox': (x, y, w, h)\n",
        "                })\n",
        "        \n",
        "        print(f\"Stage 1 complete: Found {len(flakes)} valid flakes\")\n",
        "        return img_rgb, gray, binary, flakes\n",
        "    \n",
        "    def stage2_detect_multilayers_enhanced(self, img_rgb, gray, flakes):\n",
        "        \"\"\"Stage 2: ENHANCED multilayer detection - Much more sensitive\"\"\"\n",
        "        print(f\"\\n=== STAGE 2: ENHANCED MULTILAYER DETECTION ===\")\n",
        "        \n",
        "        multilayer_flakes = []\n",
        "        \n",
        "        for flake in flakes:\n",
        "            try:\n",
        "                print(f\"\\nAnalyzing flake {flake['id']} (area: {flake['area']:.0f}px)...\")\n",
        "                \n",
        "                # Extract region of interest around the flake\n",
        "                x, y, w, h = flake['bbox']\n",
        "                \n",
        "                # Expand ROI more generously\n",
        "                margin = 15  # Increased margin\n",
        "                x1 = max(0, x - margin)\n",
        "                y1 = max(0, y - margin)\n",
        "                x2 = min(img_rgb.shape[1], x + w + margin)\n",
        "                y2 = min(img_rgb.shape[0], y + h + margin)\n",
        "                \n",
        "                roi_gray = gray[y1:y2, x1:x2]\n",
        "                roi_rgb = img_rgb[y1:y2, x1:x2]\n",
        "                \n",
        "                # Create mask for this flake\n",
        "                mask = np.zeros(gray.shape, dtype=np.uint8)\n",
        "                cv2.fillPoly(mask, [flake['contour']], 255)\n",
        "                roi_mask = mask[y1:y2, x1:x2]\n",
        "                \n",
        "                # ENHANCED: Multiple detection approaches\n",
        "                internal_structures = []\n",
        "                \n",
        "                # Method 1: Enhanced edge detection with multiple thresholds\n",
        "                edge_structures = self.detect_by_enhanced_edges(roi_gray, roi_mask, x1, y1, flake['id'])\n",
        "                internal_structures.extend(edge_structures)\n",
        "                \n",
        "                # Method 2: Multi-level intensity analysis\n",
        "                intensity_structures = self.detect_by_multilevel_intensity(roi_gray, roi_mask, x1, y1, flake['id'])\n",
        "                internal_structures.extend(intensity_structures)\n",
        "                \n",
        "                # Method 3: Texture-based detection\n",
        "                texture_structures = self.detect_by_texture_analysis(roi_gray, roi_mask, x1, y1, flake['id'])\n",
        "                internal_structures.extend(texture_structures)\n",
        "                \n",
        "                # Method 4: Gradient-based detection\n",
        "                gradient_structures = self.detect_by_gradient_analysis(roi_gray, roi_mask, x1, y1, flake['id'])\n",
        "                internal_structures.extend(gradient_structures)\n",
        "                \n",
        "                # Remove duplicates and filter\n",
        "                unique_structures = self.remove_duplicate_structures(internal_structures)\n",
        "                \n",
        "                print(f\"  Found {len(unique_structures)} internal structures\")\n",
        "                \n",
        "                if unique_structures:\n",
        "                    flake['internal_structures'] = unique_structures\n",
        "                    flake['is_multilayer'] = True\n",
        "                    flake['layer_count'] = len(unique_structures) + 1  # +1 for base layer\n",
        "                    multilayer_flakes.append(flake)\n",
        "                else:\n",
        "                    flake['is_multilayer'] = False\n",
        "                    flake['layer_count'] = 1\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing flake {flake['id']}: {str(e)}\")\n",
        "                flake['is_multilayer'] = False\n",
        "                flake['layer_count'] = 1\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\nStage 2 complete: Found {len(multilayer_flakes)} multilayer structures\")\n",
        "        print(f\"Multilayer detection rate: {len(multilayer_flakes)/len(flakes)*100:.1f}%\")\n",
        "        return multilayer_flakes\n",
        "    \n",
        "    def detect_by_enhanced_edges(self, roi_gray, roi_mask, offset_x, offset_y, flake_id):\n",
        "        \"\"\"Enhanced edge detection with multiple sensitivity levels\"\"\"\n",
        "        internal_structures = []\n",
        "        \n",
        "        try:\n",
        "            # Multiple edge detection approaches with different parameters\n",
        "            edge_params = [\n",
        "                (20, 60),   # Very sensitive\n",
        "                (30, 80),   # Medium sensitive\n",
        "                (40, 100),  # Less sensitive\n",
        "                (15, 45)    # Ultra sensitive\n",
        "            ]\n",
        "            \n",
        "            combined_edges = np.zeros_like(roi_gray)\n",
        "            \n",
        "            for low_thresh, high_thresh in edge_params:\n",
        "                edges = cv2.Canny(roi_gray, low_thresh, high_thresh)\n",
        "                edges = cv2.bitwise_and(edges, roi_mask)\n",
        "                combined_edges = cv2.bitwise_or(combined_edges, edges)\n",
        "            \n",
        "            # Multiple morphological operations\n",
        "            kernels = [\n",
        "                np.ones((2,2), np.uint8),\n",
        "                np.ones((3,3), np.uint8)\n",
        "            ]\n",
        "            \n",
        "            for kernel in kernels:\n",
        "                processed_edges = cv2.dilate(combined_edges, kernel, iterations=1)\n",
        "                processed_edges = cv2.morphologyEx(processed_edges, cv2.MORPH_CLOSE, kernel)\n",
        "                \n",
        "                # Find contours\n",
        "                contours, _ = cv2.findContours(processed_edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                \n",
        "                for contour in contours:\n",
        "                    if len(contour) < 3:\n",
        "                        continue\n",
        "                        \n",
        "                    area = cv2.contourArea(contour)\n",
        "                    \n",
        "                    # RELAXED thresholds\n",
        "                    roi_area = np.sum(roi_mask > 0)\n",
        "                    area_ratio = area / roi_area if roi_area > 0 else 0\n",
        "                    \n",
        "                    if area > self.min_internal_area and area_ratio > self.min_area_ratio:\n",
        "                        adjusted_contour = contour + [offset_x, offset_y]\n",
        "                        \n",
        "                        # More flexible shape criteria\n",
        "                        epsilon = 0.04 * cv2.arcLength(contour, True)  # More flexible approximation\n",
        "                        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                        \n",
        "                        if 3 <= len(approx) <= 12:  # More flexible vertex count\n",
        "                            internal_structures.append({\n",
        "                                'contour': adjusted_contour,\n",
        "                                'approx': approx + [offset_x, offset_y],\n",
        "                                'area': area,\n",
        "                                'vertices': len(approx),\n",
        "                                'detection_method': f'enhanced_edge_{low_thresh}_{high_thresh}',\n",
        "                                'area_ratio': area_ratio\n",
        "                            })\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  Error in enhanced edge detection: {str(e)}\")\n",
        "        \n",
        "        print(f\"  Enhanced edges found: {len(internal_structures)} structures\")\n",
        "        return internal_structures\n",
        "    \n",
        "    def detect_by_multilevel_intensity(self, roi_gray, roi_mask, offset_x, offset_y, flake_id):\n",
        "        \"\"\"Multi-level intensity analysis with different thresholds\"\"\"\n",
        "        internal_structures = []\n",
        "        \n",
        "        try:\n",
        "            masked_roi = cv2.bitwise_and(roi_gray, roi_mask)\n",
        "            if masked_roi.max() == 0:\n",
        "                return internal_structures\n",
        "            \n",
        "            mask_pixels = masked_roi[roi_mask > 0]\n",
        "            if len(mask_pixels) == 0:\n",
        "                return internal_structures\n",
        "            \n",
        "            mean_intensity = mask_pixels.mean()\n",
        "            std_intensity = mask_pixels.std()\n",
        "            \n",
        "            # Multiple intensity thresholds for different sensitivity levels\n",
        "            for intensity_drop in self.intensity_drop_levels:\n",
        "                dark_threshold = mean_intensity - intensity_drop\n",
        "                \n",
        "                dark_regions = (masked_roi < dark_threshold) & (roi_mask > 0)\n",
        "                dark_regions = dark_regions.astype(np.uint8) * 255\n",
        "                \n",
        "                # Different morphological operations for each threshold\n",
        "                kernel_sizes = [2, 3, 4] if intensity_drop <= 10 else [3, 4]\n",
        "                \n",
        "                for k_size in kernel_sizes:\n",
        "                    kernel = np.ones((k_size, k_size), np.uint8)\n",
        "                    processed = cv2.morphologyEx(dark_regions, cv2.MORPH_OPEN, kernel)\n",
        "                    processed = cv2.morphologyEx(processed, cv2.MORPH_CLOSE, kernel)\n",
        "                    \n",
        "                    contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                    \n",
        "                    for contour in contours:\n",
        "                        if len(contour) < 3:\n",
        "                            continue\n",
        "                            \n",
        "                        area = cv2.contourArea(contour)\n",
        "                        roi_area = np.sum(roi_mask > 0)\n",
        "                        area_ratio = area / roi_area if roi_area > 0 else 0\n",
        "                        \n",
        "                        # Adaptive threshold based on intensity drop\n",
        "                        min_area = self.min_internal_area * (intensity_drop / 20)  # Scale with sensitivity\n",
        "                        \n",
        "                        if area > min_area and area_ratio > self.min_area_ratio:\n",
        "                            adjusted_contour = contour + [offset_x, offset_y]\n",
        "                            \n",
        "                            epsilon = 0.04 * cv2.arcLength(contour, True)\n",
        "                            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                            \n",
        "                            if len(approx) >= 3:\n",
        "                                internal_structures.append({\n",
        "                                    'contour': adjusted_contour,\n",
        "                                    'approx': approx + [offset_x, offset_y],\n",
        "                                    'area': area,\n",
        "                                    'vertices': len(approx),\n",
        "                                    'detection_method': f'intensity_{intensity_drop}_{k_size}',\n",
        "                                    'area_ratio': area_ratio,\n",
        "                                    'intensity_drop': intensity_drop\n",
        "                                })\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  Error in multilevel intensity detection: {str(e)}\")\n",
        "        \n",
        "        print(f\"  Multi-level intensity found: {len(internal_structures)} structures\")\n",
        "        return internal_structures\n",
        "    \n",
        "    def detect_by_texture_analysis(self, roi_gray, roi_mask, offset_x, offset_y, flake_id):\n",
        "        \"\"\"Texture-based detection for subtle internal features\"\"\"\n",
        "        internal_structures = []\n",
        "        \n",
        "        try:\n",
        "            # Apply Gaussian filters with different scales\n",
        "            scales = [1.0, 1.5, 2.0]\n",
        "            \n",
        "            for scale in scales:\n",
        "                # Gaussian filtered image\n",
        "                filtered = gaussian(roi_gray, sigma=scale)\n",
        "                filtered = (filtered * 255).astype(np.uint8)\n",
        "                \n",
        "                # Difference from original (highlights texture differences)\n",
        "                diff = np.abs(roi_gray.astype(float) - filtered.astype(float))\n",
        "                diff = (diff / diff.max() * 255).astype(np.uint8) if diff.max() > 0 else diff.astype(np.uint8)\n",
        "                \n",
        "                # Apply mask\n",
        "                diff = cv2.bitwise_and(diff, roi_mask)\n",
        "                \n",
        "                # Threshold texture differences\n",
        "                texture_thresh = diff.mean() + diff.std() * 0.5\n",
        "                texture_binary = (diff > texture_thresh).astype(np.uint8) * 255\n",
        "                \n",
        "                # Clean up\n",
        "                kernel = np.ones((3,3), np.uint8)\n",
        "                texture_binary = cv2.morphologyEx(texture_binary, cv2.MORPH_CLOSE, kernel)\n",
        "                \n",
        "                contours, _ = cv2.findContours(texture_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                \n",
        "                for contour in contours:\n",
        "                    if len(contour) < 3:\n",
        "                        continue\n",
        "                        \n",
        "                    area = cv2.contourArea(contour)\n",
        "                    roi_area = np.sum(roi_mask > 0)\n",
        "                    area_ratio = area / roi_area if roi_area > 0 else 0\n",
        "                    \n",
        "                    if area > self.min_internal_area * 0.7 and area_ratio > self.min_area_ratio * 0.8:  # Slightly relaxed\n",
        "                        adjusted_contour = contour + [offset_x, offset_y]\n",
        "                        \n",
        "                        epsilon = 0.05 * cv2.arcLength(contour, True)\n",
        "                        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                        \n",
        "                        if len(approx) >= 3:\n",
        "                            internal_structures.append({\n",
        "                                'contour': adjusted_contour,\n",
        "                                'approx': approx + [offset_x, offset_y],\n",
        "                                'area': area,\n",
        "                                'vertices': len(approx),\n",
        "                                'detection_method': f'texture_{scale}',\n",
        "                                'area_ratio': area_ratio\n",
        "                            })\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  Error in texture analysis: {str(e)}\")\n",
        "        \n",
        "        print(f\"  Texture analysis found: {len(internal_structures)} structures\")\n",
        "        return internal_structures\n",
        "    \n",
        "    def detect_by_gradient_analysis(self, roi_gray, roi_mask, offset_x, offset_y, flake_id):\n",
        "        \"\"\"Gradient-based detection for edge transitions\"\"\"\n",
        "        internal_structures = []\n",
        "        \n",
        "        try:\n",
        "            # Sobel gradients\n",
        "            grad_x = cv2.Sobel(roi_gray, cv2.CV_64F, 1, 0, ksize=3)\n",
        "            grad_y = cv2.Sobel(roi_gray, cv2.CV_64F, 0, 1, ksize=3)\n",
        "            \n",
        "            # Gradient magnitude\n",
        "            grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
        "            grad_mag = (grad_mag / grad_mag.max() * 255).astype(np.uint8) if grad_mag.max() > 0 else grad_mag.astype(np.uint8)\n",
        "            \n",
        "            # Apply mask\n",
        "            grad_mag = cv2.bitwise_and(grad_mag, roi_mask)\n",
        "            \n",
        "            # Threshold gradients\n",
        "            grad_thresh = grad_mag.mean() + grad_mag.std() * 1.5\n",
        "            grad_binary = (grad_mag > grad_thresh).astype(np.uint8) * 255\n",
        "            \n",
        "            # Morphological operations\n",
        "            kernel = np.ones((2,2), np.uint8)\n",
        "            grad_binary = cv2.morphologyEx(grad_binary, cv2.MORPH_CLOSE, kernel)\n",
        "            grad_binary = cv2.dilate(grad_binary, kernel, iterations=1)\n",
        "            \n",
        "            contours, _ = cv2.findContours(grad_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            \n",
        "            for contour in contours:\n",
        "                if len(contour) < 3:\n",
        "                    continue\n",
        "                    \n",
        "                area = cv2.contourArea(contour)\n",
        "                roi_area = np.sum(roi_mask > 0)\n",
        "                area_ratio = area / roi_area if roi_area > 0 else 0\n",
        "                \n",
        "                if area > self.min_internal_area * 0.8 and area_ratio > self.min_area_ratio:\n",
        "                    adjusted_contour = contour + [offset_x, offset_y]\n",
        "                    \n",
        "                    epsilon = 0.04 * cv2.arcLength(contour, True)\n",
        "                    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                    \n",
        "                    if len(approx) >= 3:\n",
        "                        internal_structures.append({\n",
        "                            'contour': adjusted_contour,\n",
        "                            'approx': approx + [offset_x, offset_y],\n",
        "                            'area': area,\n",
        "                            'vertices': len(approx),\n",
        "                            'detection_method': 'gradient',\n",
        "                            'area_ratio': area_ratio\n",
        "                        })\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"  Error in gradient analysis: {str(e)}\")\n",
        "        \n",
        "        print(f\"  Gradient analysis found: {len(internal_structures)} structures\")\n",
        "        return internal_structures\n",
        "    \n",
        "    def remove_duplicate_structures(self, internal_structures):\n",
        "        \"\"\"Remove duplicate internal structures based on centroid distance and area similarity\"\"\"\n",
        "        if not internal_structures:\n",
        "            return []\n",
        "        \n",
        "        unique_structures = []\n",
        "        \n",
        "        for structure in internal_structures:\n",
        "            # Calculate centroid\n",
        "            M = cv2.moments(structure['contour'])\n",
        "            if M['m00'] == 0:\n",
        "                continue\n",
        "                \n",
        "            cx = M['m10'] / M['m00']\n",
        "            cy = M['m01'] / M['m00']\n",
        "            \n",
        "            # Check for duplicates\n",
        "            is_duplicate = False\n",
        "            \n",
        "            for existing in unique_structures:\n",
        "                existing_M = cv2.moments(existing['contour'])\n",
        "                if existing_M['m00'] == 0:\n",
        "                    continue\n",
        "                    \n",
        "                existing_cx = existing_M['m10'] / existing_M['m00']\n",
        "                existing_cy = existing_M['m01'] / existing_M['m00']\n",
        "                \n",
        "                # Distance check\n",
        "                distance = np.sqrt((cx - existing_cx)**2 + (cy - existing_cy)**2)\n",
        "                \n",
        "                # Area similarity check\n",
        "                area_ratio = min(structure['area'], existing['area']) / max(structure['area'], existing['area'])\n",
        "                \n",
        "                # Consider duplicate if close in position AND similar in area\n",
        "                if distance < 20 and area_ratio > 0.7:\n",
        "                    # Keep the larger structure\n",
        "                    if structure['area'] > existing['area']:\n",
        "                        unique_structures.remove(existing)\n",
        "                        unique_structures.append(structure)\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "            \n",
        "            if not is_duplicate:\n",
        "                unique_structures.append(structure)\n",
        "        \n",
        "        return unique_structures\n",
        "    \n",
        "    def stage3_calculate_twist_angles(self, multilayer_flakes):\n",
        "        \"\"\"Stage 3: Calculate twist angles for bilayer structures - SAME AS BEFORE\"\"\"\n",
        "        print(f\"\\n=== STAGE 3: TWIST ANGLE CALCULATION ===\")\n",
        "        \n",
        "        angle_results = []\n",
        "        \n",
        "        for flake in multilayer_flakes:\n",
        "            if not flake['is_multilayer'] or not flake.get('internal_structures', []):\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                # Calculate main flake orientation\n",
        "                main_vertices = flake['approx'].reshape(-1, 2)\n",
        "                main_angle = self.calculate_triangle_orientation(main_vertices)\n",
        "                \n",
        "                # Calculate angles for internal structures\n",
        "                twist_measurements = []\n",
        "                \n",
        "                for internal in flake['internal_structures']:\n",
        "                    if len(internal['approx']) >= 3:\n",
        "                        internal_vertices = internal['approx'].reshape(-1, 2)\n",
        "                        internal_angle = self.calculate_triangle_orientation(internal_vertices)\n",
        "                        \n",
        "                        # Calculate twist angle\n",
        "                        twist_angle = abs(main_angle - internal_angle)\n",
        "                        \n",
        "                        # Normalize to 0-60 degrees (considering 3-fold symmetry of MoS2)\n",
        "                        twist_angle = min(twist_angle, 180 - twist_angle)\n",
        "                        if twist_angle > 60:\n",
        "                            twist_angle = 120 - twist_angle\n",
        "                        \n",
        "                        twist_measurements.append({\n",
        "                            'internal_angle': internal_angle,\n",
        "                            'twist_angle': abs(twist_angle),\n",
        "                            'internal_area': internal['area'],\n",
        "                            'detection_method': internal.get('detection_method', 'unknown')\n",
        "                        })\n",
        "                \n",
        "                if twist_measurements:\n",
        "                    angle_results.append({\n",
        "                        'flake_id': flake['id'],\n",
        "                        'main_angle': main_angle,\n",
        "                        'twist_measurements': twist_measurements,\n",
        "                        'average_twist': np.mean([t['twist_angle'] for t in twist_measurements]),\n",
        "                        'area': flake['area'],\n",
        "                        'centroid': flake['centroid']\n",
        "                    })\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating angles for flake {flake['id']}: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"Stage 3 complete: Calculated twist angles for {len(angle_results)} bilayer structures\")\n",
        "        return angle_results\n",
        "    \n",
        "    def calculate_triangle_orientation(self, vertices):\n",
        "        \"\"\"Calculate the orientation angle of a triangular shape\"\"\"\n",
        "        if len(vertices) < 3:\n",
        "            return 0\n",
        "        \n",
        "        # Calculate centroid\n",
        "        centroid = np.mean(vertices, axis=0)\n",
        "        \n",
        "        # Find the vertex furthest from centroid (likely the apex)\n",
        "        distances = np.linalg.norm(vertices - centroid, axis=1)\n",
        "        apex_idx = np.argmax(distances)\n",
        "        apex = vertices[apex_idx]\n",
        "        \n",
        "        # Calculate angle from centroid to apex\n",
        "        angle = np.arctan2(apex[1] - centroid[1], apex[0] - centroid[0])\n",
        "        return np.degrees(angle) % 360\n",
        "    \n",
        "    def visualize_enhanced_results(self, img_rgb, flakes, multilayer_flakes, angle_results):\n",
        "        \"\"\"Enhanced visualization showing detection methods and more details\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "        \n",
        "        # Stage 1: All detected flakes\n",
        "        stage1_img = img_rgb.copy()\n",
        "        for flake in flakes:\n",
        "            color = (0, 255, 0) if flake.get('is_multilayer', False) else (255, 0, 0)\n",
        "            cv2.drawContours(stage1_img, [flake['contour']], -1, color, 2)\n",
        "            cx, cy = flake['centroid']\n",
        "            cv2.putText(stage1_img, str(flake['id']), (cx-10, cy), \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
        "        \n",
        "        axes[0,0].imshow(stage1_img)\n",
        "        axes[0,0].set_title(f'Stage 1: All Flakes ({len(flakes)}) - Green=Multilayer, Red=Single')\n",
        "        axes[0,0].axis('off')\n",
        "        \n",
        "        # Stage 2: Enhanced multilayer structures\n",
        "        stage2_img = img_rgb.copy()\n",
        "        colors_methods = {\n",
        "            'enhanced_edge': (0, 255, 255),    # Cyan\n",
        "            'intensity': (255, 0, 255),        # Magenta\n",
        "            'texture': (255, 255, 0),          # Yellow\n",
        "            'gradient': (0, 255, 0)            # Green\n",
        "        }\n",
        "        \n",
        "        for flake in multilayer_flakes:\n",
        "            # Draw main flake in white\n",
        "            cv2.drawContours(stage2_img, [flake['contour']], -1, (255, 255, 255), 3)\n",
        "            \n",
        "            # Draw internal structures with different colors by method\n",
        "            for internal in flake.get('internal_structures', []):\n",
        "                method = internal.get('detection_method', '').split('_')[0]\n",
        "                color = colors_methods.get(method, (128, 128, 128))\n",
        "                cv2.drawContours(stage2_img, [internal['contour']], -1, color, 2)\n",
        "            \n",
        "            cx, cy = flake['centroid']\n",
        "            cv2.putText(stage2_img, f\"{flake['id']}({flake['layer_count']}L)\", (cx-15, cy), \n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 2)\n",
        "        \n",
        "        axes[0,1].imshow(stage2_img)\n",
        "        axes[0,1].set_title(f'Stage 2: Enhanced Multilayer Detection ({len(multilayer_flakes)})')\n",
        "        axes[0,1].axis('off')\n",
        "        \n",
        "        # Stage 3: Twist angles with detection method info\n",
        "        stage3_img = img_rgb.copy()\n",
        "        for result in angle_results:\n",
        "            flake = next((f for f in multilayer_flakes if f['id'] == result['flake_id']), None)\n",
        "            if flake:\n",
        "                cv2.drawContours(stage3_img, [flake['contour']], -1, (255, 165, 0), 2)\n",
        "                \n",
        "                cx, cy = result['centroid']\n",
        "                angle_text = f\"{result['average_twist']:.1f}¬∞\"\n",
        "                cv2.putText(stage3_img, angle_text, (cx-25, cy+20), \n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "        \n",
        "        axes[1,0].imshow(stage3_img)\n",
        "        axes[1,0].set_title(f'Stage 3: Twist Angles ({len(angle_results)} bilayers)')\n",
        "        axes[1,0].axis('off')\n",
        "        \n",
        "        # Enhanced summary statistics\n",
        "        axes[1,1].axis('off')\n",
        "        \n",
        "        # Count detection methods\n",
        "        method_counts = {}\n",
        "        for flake in multilayer_flakes:\n",
        "            for internal in flake.get('internal_structures', []):\n",
        "                method = internal.get('detection_method', 'unknown').split('_')[0]\n",
        "                method_counts[method] = method_counts.get(method, 0) + 1\n",
        "        \n",
        "        summary_text = f\"\"\"ENHANCED ANALYSIS SUMMARY\n",
        "        \n",
        "Stage 1: Flake Detection\n",
        "‚Ä¢ Total flakes detected: {len(flakes)}\n",
        "‚Ä¢ Intensity threshold: < {self.intensity_threshold}\n",
        "‚Ä¢ Size range: {self.min_flake_area}-{self.max_flake_area} px\n",
        "\n",
        "Stage 2: Enhanced Multilayer Detection\n",
        "‚Ä¢ Multilayer flakes: {len(multilayer_flakes)}\n",
        "‚Ä¢ Detection rate: {len(multilayer_flakes)/len(flakes)*100:.1f}%\n",
        "‚Ä¢ Min internal area: {self.min_internal_area} px\n",
        "‚Ä¢ Min area ratio: {self.min_area_ratio*100:.1f}%\n",
        "\n",
        "Detection Methods Used:\n",
        "\"\"\"\n",
        "        \n",
        "        for method, count in method_counts.items():\n",
        "            summary_text += f\"‚Ä¢ {method.title()}: {count} structures\\n\"\n",
        "        \n",
        "        summary_text += f\"\\nStage 3: Twist Angle Analysis\\n‚Ä¢ Bilayer structures: {len(angle_results)}\\n\"\n",
        "        \n",
        "        if angle_results:\n",
        "            all_angles = [r['average_twist'] for r in angle_results]\n",
        "            summary_text += f\"\"\"‚Ä¢ Mean twist angle: {np.mean(all_angles):.1f}¬∞\n",
        "‚Ä¢ Angle range: {np.min(all_angles):.1f}¬∞ - {np.max(all_angles):.1f}¬∞\n",
        "‚Ä¢ Standard deviation: {np.std(all_angles):.1f}¬∞\n",
        "\n",
        "Color Legend (Stage 2):\n",
        "‚Ä¢ Cyan: Edge detection\n",
        "‚Ä¢ Magenta: Intensity analysis  \n",
        "‚Ä¢ Yellow: Texture analysis\n",
        "‚Ä¢ Green: Gradient analysis\"\"\"\n",
        "        \n",
        "        axes[1,1].text(0.05, 0.95, summary_text, transform=axes[1,1].transAxes, \n",
        "                      verticalalignment='top', fontsize=9, fontfamily='monospace')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "    \n",
        "    def process_enhanced_pipeline(self, image_path):\n",
        "        \"\"\"Run the enhanced 3-stage analysis pipeline\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ENHANCED MoS2 ANALYSIS PIPELINE\")\n",
        "        print(f\"Image: {Path(image_path).name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Stage 1: Detect flakes (same as before - working well)\n",
        "        img_rgb, gray, binary, flakes = self.stage1_detect_flakes(image_path)\n",
        "        \n",
        "        # Stage 2: Enhanced multilayer detection\n",
        "        multilayer_flakes = self.stage2_detect_multilayers_enhanced(img_rgb, gray, flakes)\n",
        "        \n",
        "        # Stage 3: Calculate twist angles\n",
        "        angle_results = self.stage3_calculate_twist_angles(multilayer_flakes)\n",
        "        \n",
        "        # Enhanced visualization\n",
        "        fig = self.visualize_enhanced_results(img_rgb, flakes, multilayer_flakes, angle_results)\n",
        "        \n",
        "        return {\n",
        "            'image': img_rgb,\n",
        "            'flakes': flakes,\n",
        "            'multilayer_flakes': multilayer_flakes,\n",
        "            'angle_results': angle_results,\n",
        "            'figure': fig\n",
        "        }\n",
        "\n",
        "# Initialize the enhanced pipeline\n",
        "pipeline = EnhancedMoS2Pipeline()\n",
        "print(\"‚úì Enhanced MoS2 Analysis Pipeline initialized\")\n",
        "print(f\"‚úì Enhanced multilayer detection parameters:\")\n",
        "print(f\"  ‚Ä¢ Min internal area: {pipeline.min_internal_area} px (reduced)\")\n",
        "print(f\"  ‚Ä¢ Min area ratio: {pipeline.min_area_ratio*100:.1f}% (reduced)\")\n",
        "print(f\"  ‚Ä¢ Intensity sensitivity levels: {pipeline.intensity_drop_levels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_enhanced_pipeline"
      },
      "outputs": [],
      "source": [
        "# Run enhanced pipeline on all images\n",
        "image_dir = Path('/content/images')\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.bmp']\n",
        "\n",
        "image_files = []\n",
        "for ext in image_extensions:\n",
        "    image_files.extend(image_dir.glob(f'*{ext}'))\n",
        "    image_files.extend(image_dir.glob(f'*{ext.upper()}'))\n",
        "\n",
        "print(f\"Found {len(image_files)} images to process\\n\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for image_path in image_files:\n",
        "    try:\n",
        "        # Run enhanced pipeline\n",
        "        results = pipeline.process_enhanced_pipeline(str(image_path))\n",
        "        \n",
        "        # Save enhanced visualization\n",
        "        plt.figure(results['figure'].number)\n",
        "        plt.savefig(f'/content/results/{image_path.stem}_enhanced_analysis.png', \n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # Print detailed results with detection method breakdown\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ENHANCED DETAILED RESULTS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        flakes = results['flakes']\n",
        "        multilayer_flakes = results['multilayer_flakes']\n",
        "        angle_results = results['angle_results']\n",
        "        \n",
        "        print(f\"\\nFLAKE SUMMARY:\")\n",
        "        for flake in flakes:\n",
        "            layer_info = f\" ({flake['layer_count']}L)\" if flake.get('is_multilayer') else \" (1L)\"\n",
        "            multilayer_indicator = \" *** MULTILAYER ***\" if flake.get('is_multilayer') else \"\"\n",
        "            print(f\"  Flake {flake['id']}: Area={flake['area']:.0f}px, \"\n",
        "                  f\"Vertices={flake['vertices']}, Solidity={flake['solidity']:.2f}{layer_info}{multilayer_indicator}\")\n",
        "        \n",
        "        if multilayer_flakes:\n",
        "            print(f\"\\nMULTILAYER DETAILS:\")\n",
        "            for flake in multilayer_flakes:\n",
        "                print(f\"\\n  Flake {flake['id']} - {len(flake.get('internal_structures', []))} internal structures:\")\n",
        "                for i, internal in enumerate(flake.get('internal_structures', [])):\n",
        "                    method = internal.get('detection_method', 'unknown')\n",
        "                    area_ratio = internal.get('area_ratio', 0) * 100\n",
        "                    print(f\"    Structure {i+1}: {internal['area']:.0f}px ({area_ratio:.1f}% of flake), \"\n",
        "                          f\"Method: {method}, Vertices: {internal['vertices']}\")\n",
        "        \n",
        "        if angle_results:\n",
        "            print(f\"\\nTWIST ANGLES:\")\n",
        "            for result in angle_results:\n",
        "                print(f\"\\n  Flake {result['flake_id']}: Average = {result['average_twist']:.1f}¬∞ \"\n",
        "                      f\"(from {len(result['twist_measurements'])} measurements)\")\n",
        "                for i, measurement in enumerate(result['twist_measurements']):\n",
        "                    method = measurement.get('detection_method', 'unknown')\n",
        "                    print(f\"    Measurement {i+1}: {measurement['twist_angle']:.1f}¬∞ \"\n",
        "                          f\"(area: {measurement['internal_area']:.0f}px, method: {method})\")\n",
        "        \n",
        "        # Save enhanced JSON results\n",
        "        json_data = {\n",
        "            'filename': image_path.name,\n",
        "            'analysis_summary': {\n",
        "                'total_flakes': len(flakes),\n",
        "                'multilayer_flakes': len(multilayer_flakes),\n",
        "                'bilayer_structures': len(angle_results),\n",
        "                'detection_rate_percent': len(multilayer_flakes)/len(flakes)*100 if flakes else 0\n",
        "            },\n",
        "            'detection_parameters': {\n",
        "                'intensity_threshold': pipeline.intensity_threshold,\n",
        "                'min_internal_area': pipeline.min_internal_area,\n",
        "                'min_area_ratio': pipeline.min_area_ratio,\n",
        "                'intensity_drop_levels': pipeline.intensity_drop_levels\n",
        "            },\n",
        "            'flake_details': [],\n",
        "            'multilayer_details': [],\n",
        "            'twist_angle_data': []\n",
        "        }\n",
        "        \n",
        "        # Save flake details\n",
        "        for flake in flakes:\n",
        "            flake_data = {\n",
        "                'id': flake['id'],\n",
        "                'area': float(flake['area']),\n",
        "                'vertices': int(flake['vertices']),\n",
        "                'solidity': float(flake['solidity']),\n",
        "                'aspect_ratio': float(flake['aspect_ratio']),\n",
        "                'is_multilayer': bool(flake.get('is_multilayer', False)),\n",
        "                'layer_count': int(flake.get('layer_count', 1)),\n",
        "                'centroid': flake['centroid']\n",
        "            }\n",
        "            json_data['flake_details'].append(flake_data)\n",
        "        \n",
        "        # Save multilayer details with detection methods\n",
        "        for flake in multilayer_flakes:\n",
        "            multilayer_data = {\n",
        "                'flake_id': flake['id'],\n",
        "                'internal_structures': []\n",
        "            }\n",
        "            \n",
        "            for internal in flake.get('internal_structures', []):\n",
        "                internal_data = {\n",
        "                    'area': float(internal['area']),\n",
        "                    'vertices': int(internal['vertices']),\n",
        "                    'detection_method': internal.get('detection_method', 'unknown'),\n",
        "                    'area_ratio': float(internal.get('area_ratio', 0))\n",
        "                }\n",
        "                multilayer_data['internal_structures'].append(internal_data)\n",
        "            \n",
        "            json_data['multilayer_details'].append(multilayer_data)\n",
        "        \n",
        "        # Save twist angle data\n",
        "        for result in angle_results:\n",
        "            angle_data = {\n",
        "                'flake_id': result['flake_id'],\n",
        "                'main_angle': float(result['main_angle']),\n",
        "                'average_twist': float(result['average_twist']),\n",
        "                'individual_measurements': [\n",
        "                    {\n",
        "                        'twist_angle': float(m['twist_angle']),\n",
        "                        'internal_area': float(m['internal_area']),\n",
        "                        'detection_method': m.get('detection_method', 'unknown')\n",
        "                    } for m in result['twist_measurements']\n",
        "                ]\n",
        "            }\n",
        "            json_data['twist_angle_data'].append(angle_data)\n",
        "        \n",
        "        # Save to file\n",
        "        with open(f'/content/results/{image_path.stem}_enhanced_results.json', 'w') as f:\n",
        "            json.dump(json_data, f, indent=2)\n",
        "        \n",
        "        all_results.append(json_data)\n",
        "        print(f\"\\n‚úì Enhanced results saved for {image_path.name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {image_path.name}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "# Enhanced final summary\n",
        "if all_results:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ENHANCED FINAL SUMMARY - ALL IMAGES\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    total_flakes = sum(r['analysis_summary']['total_flakes'] for r in all_results)\n",
        "    total_multilayer = sum(r['analysis_summary']['multilayer_flakes'] for r in all_results)\n",
        "    total_bilayer = sum(r['analysis_summary']['bilayer_structures'] for r in all_results)\n",
        "    \n",
        "    print(f\"Images processed: {len(all_results)}\")\n",
        "    print(f\"Total flakes detected: {total_flakes}\")\n",
        "    print(f\"Total multilayer structures: {total_multilayer}\")\n",
        "    print(f\"Total bilayer structures with twist angles: {total_bilayer}\")\n",
        "    \n",
        "    if total_flakes > 0:\n",
        "        print(f\"ENHANCED multilayer detection rate: {total_multilayer/total_flakes*100:.1f}%\")\n",
        "        print(f\"Improvement from 3.8% to {total_multilayer/total_flakes*100:.1f}%\")\n",
        "    \n",
        "    # Method effectiveness analysis\n",
        "    method_stats = {}\n",
        "    for result in all_results:\n",
        "        for multilayer in result['multilayer_details']:\n",
        "            for structure in multilayer['internal_structures']:\n",
        "                method = structure['detection_method'].split('_')[0]\n",
        "                method_stats[method] = method_stats.get(method, 0) + 1\n",
        "    \n",
        "    if method_stats:\n",
        "        print(f\"\\nDetection method effectiveness:\")\n",
        "        for method, count in sorted(method_stats.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {method.title()}: {count} structures detected\")\n",
        "    \n",
        "    # Collect all twist angles\n",
        "    all_twist_angles = []\n",
        "    for result in all_results:\n",
        "        for angle_data in result['twist_angle_data']:\n",
        "            all_twist_angles.append(angle_data['average_twist'])\n",
        "    \n",
        "    if all_twist_angles:\n",
        "        print(f\"\\nEnhanced twist angle statistics:\")\n",
        "        print(f\"  Total measurements: {len(all_twist_angles)}\")\n",
        "        print(f\"  Mean: {np.mean(all_twist_angles):.1f}¬∞\")\n",
        "        print(f\"  Median: {np.median(all_twist_angles):.1f}¬∞\")\n",
        "        print(f\"  Range: {np.min(all_twist_angles):.1f}¬∞ - {np.max(all_twist_angles):.1f}¬∞\")\n",
        "        print(f\"  Standard deviation: {np.std(all_twist_angles):.1f}¬∞\")\n",
        "    \n",
        "    # Save enhanced overall summary\n",
        "    with open('/content/results/enhanced_pipeline_summary.json', 'w') as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n‚úì Enhanced pipeline results saved to /content/results/\")\n",
        "    print(f\"\\nüéØ Expected improvement: Should now detect flakes #11, #14, and other multilayer candidates!\")\nelse:\n",
        "    print(\"No images were successfully processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhanced_guide"
      },
      "source": [
        "## Enhanced Results Interpretation Guide\n",
        "\n",
        "### Major Improvements in Stage 2:\n",
        "1. **4x More Sensitive Detection**:\n",
        "   - Min area ratio reduced: 5% ‚Üí 2%\n",
        "   - Min internal area reduced: 100px ‚Üí 80px\n",
        "   - Multiple intensity thresholds: [5, 10, 15, 20]\n",
        "\n",
        "2. **Multiple Detection Methods**:\n",
        "   - **Enhanced Edge Detection**: 4 different Canny thresholds\n",
        "   - **Multi-level Intensity**: Multiple darkness thresholds\n",
        "   - **Texture Analysis**: Gaussian filtering for subtle features\n",
        "   - **Gradient Analysis**: Sobel gradients for transitions\n",
        "\n",
        "3. **Advanced Duplicate Removal**:\n",
        "   - Centroid distance + area similarity checks\n",
        "   - Keeps the largest/best detection\n",
        "\n",
        "### Expected Results:\n",
        "- **Detection Rate**: Should improve from 3.8% to 15-30%+\n",
        "- **Target Flakes**: Should now detect #11 (120¬∞) and #14 (60¬∞)\n",
        "- **Color Coding**: Different methods shown in different colors\n",
        "\n",
        "### Output Files:\n",
        "- **`[filename]_enhanced_analysis.png`**: Color-coded detection methods\n",
        "- **`[filename]_enhanced_results.json`**: Detailed method breakdown\n",
        "- **`enhanced_pipeline_summary.json`**: Complete analysis summary\n",
        "\n",
        "### Quality Check:\n",
        "If detection rate is still too low, you can further adjust:\n",
        "- `min_area_ratio` (currently 0.02)\n",
        "- `min_internal_area` (currently 80)\n",
        "- Add more `intensity_drop_levels`\n"
      ]
    }
  ]
}